{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c737281",
   "metadata": {},
   "source": [
    "# Putting the G in RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bb6e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from supabase import create_client, Client\n",
    "import datetime\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from typing import Dict, Optional, Any\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fac3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "url: str = os.getenv(\"SUPABASE_URL\")\n",
    "key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bc78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the API key from user data\n",
    "openai_client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc7be49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone_key = os.environ.get('PINECONE_API_KEY')\n",
    "INDEX_NAME = 'semantic-search-rag-index'\n",
    "ENGINE = 'text-embedding-3-small'\n",
    "NAMESPACE = 'default'\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=pinecone_key\n",
    ")\n",
    "\n",
    "# helper functions to get lists of embeddings from the OpenAI API\n",
    "def get_embedding(text, engine=ENGINE):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=engine\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "len(get_embedding('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637c14f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x21094c83b60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the index as a variable\n",
    "index = pc.Index(name=INDEX_NAME)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827c0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_from_pinecone(query, top_k=1, include_metadata=True):\n",
    "    # get embedding from THE SAME embedder as the documents\n",
    "    query_embedding = get_embedding(query, engine=ENGINE)\n",
    "\n",
    "    return index.query(\n",
    "      vector=query_embedding,\n",
    "      top_k=top_k,\n",
    "      namespace=NAMESPACE,\n",
    "      include_metadata=include_metadata   # gets the metadata (dates, text, etc)\n",
    "    ).get('matches')\n",
    "\n",
    "len(query_from_pinecone('KulturHighlights in Munchen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc42cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '6ad9eeafc1a442c0a4c67b832832e3bf',\n",
       "  'metadata': {'date_uploaded': '2025-05-21T12:43:59.666136',\n",
       "               'text': 'us KulturHighlights Perlen der Münchner Kultur '\n",
       "                       'Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte '\n",
       "                       'in allen Preislagen buchen Hier im Überblick '\n",
       "                       'Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks '\n",
       "                       'So schön ist München München mal anders Eure '\n",
       "                       'Stadtführung durch München mit oder ohne Guide LGBTIQ '\n",
       "                       'in München Tipps und Informationen für die queere '\n",
       "                       'Community Sehenswerte UBahnStationen Kunstwerke im '\n",
       "                       'Untergrund Sehenswerte UBahnhöfe Immer einen Besuch '\n",
       "                       'wert Museen in München Die wichtigsten Museen und '\n",
       "                       'Galerien Münchens Olympiapark Erholen, Staunen, Erleben '\n",
       "                       'im weitläufigen Park Besucherservice Tierpark '\n",
       "                       'Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen '\n",
       "                       'und Anfahrt ServiceTipps für den MünchenBesuch MVG '\n",
       "                       'Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, '\n",
       "                       'LiveAuskunft, Mieträder Die Infos Touristeninformation '\n",
       "                       'Die wichtigsten InfoPoints Hier gibts Hinweise für '\n",
       "                       'Touristen Stadtrundfahrten in München. Eine Übersicht '\n",
       "                       'der Anbieter Stadtführungen Bei diesen Anbietern wird '\n",
       "                       'die Städtetour zum Erlebnis Umzugsunternehmen in '\n",
       "                       'München und Umgebung',\n",
       "               'url': 'https://www.muenchen.de/sehenswuerdigkeiten/tourismus'},\n",
       "  'score': 0.698859096,\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_from_pinecone('KulturHighlights in Munchen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51ccc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_ANSWER_TOKEN = \"Assistant Response:\"\n",
    "STOP = '[END]'\n",
    "PROMPT_TEMPLATE = \"\"\"Today is {today} and you can retrieve information from a database. Respond to the user's input as best as you can.\n",
    "\n",
    "Here is an example of the conversation format:\n",
    "\n",
    "[START]\n",
    "User Input: the input question you must answer\n",
    "Context: retrieved context from the database\n",
    "Context URL: context url\n",
    "Context Score : a score from 0 - 1 of how strong the information is a match\n",
    "Assistant Thought: This context has sufficient information to answer the question.\n",
    "Assistant Response: your final answer to the original input question which could be I don't have sufficient information to answer the question.\n",
    "[END]\n",
    "[START]\n",
    "User Input: another input question you must answer\n",
    "Context: more retrieved context from the database\n",
    "Context URL: context url\n",
    "Context Score : another score from 0 - 1 of how strong the information is a match\n",
    "Assistant Thought: This context does not have sufficient information to answer the question.\n",
    "Assistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n",
    "[END]\n",
    "[START]\n",
    "User Input: another input question you must answer\n",
    "Context: more retrieved context from the database\n",
    "Context URL: context url\n",
    "Context Score : another score from 0 - 1 of how strong the information is a match\n",
    "Assistant Thought: A previous piece of context has the answer to this question\n",
    "Assistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n",
    "[END]\n",
    "[START]\n",
    "User Input: another input question you must answer\n",
    "Context: NO CONTEXT FOUND\n",
    "Context URL: NONE\n",
    "Context Score : 0\n",
    "Assistant Thought: We either could not find something or we don't need to look something up\n",
    "Assistant Response: I'm sorry I don't know.\n",
    "[END]\n",
    "\n",
    "Begin:\n",
    "\n",
    "{running_convo}\n",
    "\"\"\"\n",
    "\n",
    "class RagBot(BaseModel):\n",
    "    llm: Any\n",
    "    prompt_template: str = PROMPT_TEMPLATE\n",
    "    stop_pattern: List[str] = [STOP]\n",
    "    user_inputs: List[str] = []\n",
    "    ai_responses: List[str] = []\n",
    "    contexts: List[Tuple[str, float]] = []\n",
    "    verbose: bool = False\n",
    "    threshold: float = 0.6\n",
    "\n",
    "    def query_from_pinecone(self, query, top_k=2, include_metadata=True):\n",
    "        return query_from_pinecone(query, top_k, include_metadata)\n",
    "\n",
    "    @property\n",
    "    def running_convo(self):\n",
    "        convo = ''\n",
    "        for index in range(len(self.user_inputs)):\n",
    "            convo += f'[START]\\nUser Input: {self.user_inputs[index]}\\n'\n",
    "            convo += f'Context: {self.contexts[index][0]}\\nContext URL: {self.contexts[index][1]}\\nContext Score: {self.contexts[index][2]}\\n'\n",
    "            if len(self.ai_responses) > index:\n",
    "                convo += self.ai_responses[index]\n",
    "                convo += '\\n[END]\\n'\n",
    "        return convo.strip()\n",
    "\n",
    "    def run(self, question: str):\n",
    "        self.user_inputs.append(question)\n",
    "        top_response = self.query_from_pinecone(question)[0]\n",
    "        if self.verbose:\n",
    "            print(top_response['score'])\n",
    "        if top_response['score'] >= self.threshold:\n",
    "            self.contexts.append(\n",
    "                (top_response['metadata']['text'], top_response['metadata']['url'], top_response['score']))\n",
    "        else:\n",
    "            self.contexts.append(('NO CONTEXT FOUND', 'NONE', 0))\n",
    "\n",
    "        prompt = self.prompt_template.format(  # behold, the augmentation\n",
    "                today = datetime.date.today(),\n",
    "                running_convo=self.running_convo\n",
    "        )\n",
    "        if self.verbose:\n",
    "            print('--------')\n",
    "            print('PROMPT')\n",
    "            print('--------')\n",
    "            print(prompt)\n",
    "            print('--------')\n",
    "            print('END PROMPT')\n",
    "            print('--------')\n",
    "        generated = self.llm.generate(prompt, stop=self.stop_pattern)\n",
    "        if self.verbose:\n",
    "            print('--------')\n",
    "            print('GENERATED')\n",
    "            print('--------')\n",
    "            print(generated)\n",
    "            print('--------')\n",
    "            print('END GENERATED')\n",
    "            print('--------')\n",
    "        self.ai_responses.append(generated)\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            generated = generated.split(FINAL_ANSWER_TOKEN)[-1]\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af4744",
   "metadata": {},
   "source": [
    "## Using OpenAI as Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4340a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the Chat Language Model\n",
    "class OpenAIChatLLM(BaseModel):\n",
    "    model: str = 'gpt-4o'  # Default model to use\n",
    "    temperature: float = 0.5  # Default temperature for generating responses\n",
    "\n",
    "    # Method to generate a response from the model based on the provided prompt\n",
    "    def generate(self, prompt: str, stop: List[str] = None):\n",
    "        # Create a completion request to the OpenAI API with the given parameters\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=self.temperature,\n",
    "            stop=stop\n",
    "        )\n",
    "\n",
    "        # Insert the details of the prompt and response into the 'cost_projecting' table in Supabase\n",
    "        supabase.table('cost_projecting').insert({\n",
    "            'prompt': prompt,\n",
    "            'response': response.choices[0].message.content,\n",
    "            'input_tokens': response.usage.prompt_tokens,\n",
    "            'output_tokens': response.usage.completion_tokens,\n",
    "            'model': self.model,\n",
    "            'inference_params': {\n",
    "                'temperature': self.temperature,\n",
    "                'stop': stop\n",
    "            },\n",
    "            'is_openai': True,\n",
    "            'app': 'RAG'\n",
    "        }).execute()\n",
    "\n",
    "        # Return the generated response content\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca046ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = OpenAIChatLLM()\n",
    "c.generate('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9732482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "openai_rag_bot = RagBot(llm=OpenAIChatLLM(temperature=0, model='gpt-4o-mini'), stop_pattern=['[END]'], verbose=False)\n",
    "print(openai_rag_bot.run('KulturHighlights in Munchen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe37929f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry I don't know.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(openai_rag_bot.run('which city again?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dcde7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]\n",
      "User Input: KulturHighlights in Munchen\n",
      "Context: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\n",
      "Context URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\n",
      "Context Score: 0.698859096\n",
      "Assistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \n",
      "Assistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n",
      "\n",
      "[END]\n",
      "[START]\n",
      "User Input: which city again?\n",
      "Context: NO CONTEXT FOUND\n",
      "Context URL: NONE\n",
      "Context Score: 0\n",
      "Assistant Thought: We either could not find something or we don't need to look something up.  \n",
      "Assistant Response: I'm sorry I don't know.  \n",
      "\n",
      "[END]\n"
     ]
    }
   ],
   "source": [
    "print(openai_rag_bot.running_convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269b046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You're welcome! If you have any more questions, feel free to ask.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(openai_rag_bot.run('Amazing, thanks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea6d509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]\n",
      "User Input: KulturHighlights in Munchen\n",
      "Context: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\n",
      "Context URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\n",
      "Context Score: 0.698859096\n",
      "Assistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \n",
      "Assistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n",
      "\n",
      "[END]\n",
      "[START]\n",
      "User Input: which city again?\n",
      "Context: NO CONTEXT FOUND\n",
      "Context URL: NONE\n",
      "Context Score: 0\n",
      "Assistant Thought: We either could not find something or we don't need to look something up.  \n",
      "Assistant Response: I'm sorry I don't know.  \n",
      "\n",
      "[END]\n",
      "[START]\n",
      "User Input: Amazing, thanks\n",
      "Context: NO CONTEXT FOUND\n",
      "Context URL: NONE\n",
      "Context Score: 0\n",
      "Assistant Thought: We either could not find something or we don't need to look something up.  \n",
      "Assistant Response: You're welcome! If you have any more questions, feel free to ask.  \n",
      "\n",
      "\n",
      "[END]\n"
     ]
    }
   ],
   "source": [
    "print(openai_rag_bot.running_convo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cc8212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "created_at",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "response",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "input_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "output_tokens",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "inference_params",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "is_openai",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "app",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "created_at",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "207f9637-f05a-4ca5-b1b7-e5bd88cf66c4",
       "rows": [
        [
         "2025-05-21 09:28:20.378074+00:00",
         "27",
         "Today is 2025-05-21 and you can retrieve information from a database. Respond to the user's input as best as you can.\n\nHere is an example of the conversation format:\n\n[START]\nUser Input: the input question you must answer\nContext: retrieved context from the database\nContext URL: context url\nContext Score : a score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context has sufficient information to answer the question.\nAssistant Response: your final answer to the original input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context does not have sufficient information to answer the question.\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: A previous piece of context has the answer to this question\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score : 0\nAssistant Thought: We either could not find something or we don't need to look something up\nAssistant Response: I'm sorry I don't know.\n[END]\n\nBegin:\n\n[START]\nUser Input: KulturHighlights in Munchen\nContext: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\nContext URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\nContext Score: 0.698859096\nAssistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \nAssistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n\n[END]\n[START]\nUser Input: which city again?\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score: 0\nAssistant Thought: We either could not find something or we don't need to look something up.  \nAssistant Response: I'm sorry I don't know.  \n\n[END]\n[START]\nUser Input: Amazing, thanks\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score: 0\n",
         "Assistant Thought: We either could not find something or we don't need to look something up.  \nAssistant Response: You're welcome! If you have any more questions, feel free to ask.  \n\n",
         "869",
         "41",
         "gpt-4o-mini",
         "{'stop': ['[END]'], 'temperature': 0.0}",
         "True",
         "RAG",
         "2025-05-21T09:28:20.378074+00:00"
        ],
        [
         "2025-05-21 10:49:25.181414+00:00",
         "28",
         "hi",
         "Hello! How can I assist you today?",
         "8",
         "9",
         "gpt-4o",
         "{'stop': None, 'temperature': 0.5}",
         "True",
         "RAG",
         "2025-05-21T10:49:25.181414+00:00"
        ],
        [
         "2025-05-21 10:49:28.188575+00:00",
         "29",
         "Today is 2025-05-21 and you can retrieve information from a database. Respond to the user's input as best as you can.\n\nHere is an example of the conversation format:\n\n[START]\nUser Input: the input question you must answer\nContext: retrieved context from the database\nContext URL: context url\nContext Score : a score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context has sufficient information to answer the question.\nAssistant Response: your final answer to the original input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context does not have sufficient information to answer the question.\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: A previous piece of context has the answer to this question\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score : 0\nAssistant Thought: We either could not find something or we don't need to look something up\nAssistant Response: I'm sorry I don't know.\n[END]\n\nBegin:\n\n[START]\nUser Input: KulturHighlights in Munchen\nContext: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\nContext URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\nContext Score: 0.698859096\n",
         "Assistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \nAssistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n",
         "649",
         "132",
         "gpt-4o-mini",
         "{'stop': ['[END]'], 'temperature': 0.0}",
         "True",
         "RAG",
         "2025-05-21T10:49:28.188575+00:00"
        ],
        [
         "2025-05-21 10:49:29.855939+00:00",
         "30",
         "Today is 2025-05-21 and you can retrieve information from a database. Respond to the user's input as best as you can.\n\nHere is an example of the conversation format:\n\n[START]\nUser Input: the input question you must answer\nContext: retrieved context from the database\nContext URL: context url\nContext Score : a score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context has sufficient information to answer the question.\nAssistant Response: your final answer to the original input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context does not have sufficient information to answer the question.\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: A previous piece of context has the answer to this question\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score : 0\nAssistant Thought: We either could not find something or we don't need to look something up\nAssistant Response: I'm sorry I don't know.\n[END]\n\nBegin:\n\n[START]\nUser Input: KulturHighlights in Munchen\nContext: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\nContext URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\nContext Score: 0.698859096\nAssistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \nAssistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n\n[END]\n[START]\nUser Input: which city again?\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score: 0\n",
         "Assistant Thought: We either could not find something or we don't need to look something up.  \nAssistant Response: I'm sorry I don't know.  \n",
         "809",
         "32",
         "gpt-4o-mini",
         "{'stop': ['[END]'], 'temperature': 0.0}",
         "True",
         "RAG",
         "2025-05-21T10:49:29.855939+00:00"
        ],
        [
         "2025-05-21 10:49:31.534413+00:00",
         "31",
         "Today is 2025-05-21 and you can retrieve information from a database. Respond to the user's input as best as you can.\n\nHere is an example of the conversation format:\n\n[START]\nUser Input: the input question you must answer\nContext: retrieved context from the database\nContext URL: context url\nContext Score : a score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context has sufficient information to answer the question.\nAssistant Response: your final answer to the original input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: This context does not have sufficient information to answer the question.\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: more retrieved context from the database\nContext URL: context url\nContext Score : another score from 0 - 1 of how strong the information is a match\nAssistant Thought: A previous piece of context has the answer to this question\nAssistant Response: your final answer to the second input question which could be I don't have sufficient information to answer the question.\n[END]\n[START]\nUser Input: another input question you must answer\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score : 0\nAssistant Thought: We either could not find something or we don't need to look something up\nAssistant Response: I'm sorry I don't know.\n[END]\n\nBegin:\n\n[START]\nUser Input: KulturHighlights in Munchen\nContext: us KulturHighlights Perlen der Münchner Kultur Konzertsäle, Museen, Theater HotelÜbersicht Unterkünfte in allen Preislagen buchen Hier im Überblick Sehenswürdigkeiten Bauwerke, Kirchen, Schlösser, Parks So schön ist München München mal anders Eure Stadtführung durch München mit oder ohne Guide LGBTIQ in München Tipps und Informationen für die queere Community Sehenswerte UBahnStationen Kunstwerke im Untergrund Sehenswerte UBahnhöfe Immer einen Besuch wert Museen in München Die wichtigsten Museen und Galerien Münchens Olympiapark Erholen, Staunen, Erleben im weitläufigen Park Besucherservice Tierpark Hellabrunn Infos zu Öffnungszeiten, Eintrittspreisen und Anfahrt ServiceTipps für den MünchenBesuch MVG Infos zu UBahn, Bus und Tram Tickets, Fahrpläne, LiveAuskunft, Mieträder Die Infos Touristeninformation Die wichtigsten InfoPoints Hier gibts Hinweise für Touristen Stadtrundfahrten in München. Eine Übersicht der Anbieter Stadtführungen Bei diesen Anbietern wird die Städtetour zum Erlebnis Umzugsunternehmen in München und Umgebung\nContext URL: https://www.muenchen.de/sehenswuerdigkeiten/tourismus\nContext Score: 0.698859096\nAssistant Thought: This context has sufficient information to answer the question about cultural highlights in Munich.  \nAssistant Response: München bietet eine Vielzahl an Kulturhighlights, darunter beeindruckende Museen, Theater und Konzertsäle. Zu den wichtigsten Museen zählen die Alte Pinakothek, die Neue Pinakothek und das Deutsche Museum. Der Olympiapark ist ein weiterer beliebter Ort, um sich zu erholen und die Architektur zu bewundern. Für weitere Informationen über Sehenswürdigkeiten und kulturelle Angebote in München, besuchen Sie bitte [diese Seite](https://www.muenchen.de/sehenswuerdigkeiten/tourismus).  \n\n[END]\n[START]\nUser Input: which city again?\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score: 0\nAssistant Thought: We either could not find something or we don't need to look something up.  \nAssistant Response: I'm sorry I don't know.  \n\n[END]\n[START]\nUser Input: Amazing, thanks\nContext: NO CONTEXT FOUND\nContext URL: NONE\nContext Score: 0\n",
         "Assistant Thought: We either could not find something or we don't need to look something up.  \nAssistant Response: You're welcome! If you have any more questions, feel free to ask.  \n\n",
         "869",
         "41",
         "gpt-4o-mini",
         "{'stop': ['[END]'], 'temperature': 0.0}",
         "True",
         "RAG",
         "2025-05-21T10:49:31.534413+00:00"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>model</th>\n",
       "      <th>inference_params</th>\n",
       "      <th>is_openai</th>\n",
       "      <th>app</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-05-21 09:28:20.378074+00:00</th>\n",
       "      <td>27</td>\n",
       "      <td>Today is 2025-05-21 and you can retrieve infor...</td>\n",
       "      <td>Assistant Thought: We either could not find so...</td>\n",
       "      <td>869</td>\n",
       "      <td>41</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "      <td>2025-05-21T09:28:20.378074+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-21 10:49:25.181414+00:00</th>\n",
       "      <td>28</td>\n",
       "      <td>hi</td>\n",
       "      <td>Hello! How can I assist you today?</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>{'stop': None, 'temperature': 0.5}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "      <td>2025-05-21T10:49:25.181414+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-21 10:49:28.188575+00:00</th>\n",
       "      <td>29</td>\n",
       "      <td>Today is 2025-05-21 and you can retrieve infor...</td>\n",
       "      <td>Assistant Thought: This context has sufficient...</td>\n",
       "      <td>649</td>\n",
       "      <td>132</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "      <td>2025-05-21T10:49:28.188575+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-21 10:49:29.855939+00:00</th>\n",
       "      <td>30</td>\n",
       "      <td>Today is 2025-05-21 and you can retrieve infor...</td>\n",
       "      <td>Assistant Thought: We either could not find so...</td>\n",
       "      <td>809</td>\n",
       "      <td>32</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "      <td>2025-05-21T10:49:29.855939+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-21 10:49:31.534413+00:00</th>\n",
       "      <td>31</td>\n",
       "      <td>Today is 2025-05-21 and you can retrieve infor...</td>\n",
       "      <td>Assistant Thought: We either could not find so...</td>\n",
       "      <td>869</td>\n",
       "      <td>41</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "      <td>{'stop': ['[END]'], 'temperature': 0.0}</td>\n",
       "      <td>True</td>\n",
       "      <td>RAG</td>\n",
       "      <td>2025-05-21T10:49:31.534413+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "created_at                             \n",
       "2025-05-21 09:28:20.378074+00:00  27   \n",
       "2025-05-21 10:49:25.181414+00:00  28   \n",
       "2025-05-21 10:49:28.188575+00:00  29   \n",
       "2025-05-21 10:49:29.855939+00:00  30   \n",
       "2025-05-21 10:49:31.534413+00:00  31   \n",
       "\n",
       "                                                                             prompt  \\\n",
       "created_at                                                                            \n",
       "2025-05-21 09:28:20.378074+00:00  Today is 2025-05-21 and you can retrieve infor...   \n",
       "2025-05-21 10:49:25.181414+00:00                                                 hi   \n",
       "2025-05-21 10:49:28.188575+00:00  Today is 2025-05-21 and you can retrieve infor...   \n",
       "2025-05-21 10:49:29.855939+00:00  Today is 2025-05-21 and you can retrieve infor...   \n",
       "2025-05-21 10:49:31.534413+00:00  Today is 2025-05-21 and you can retrieve infor...   \n",
       "\n",
       "                                                                           response  \\\n",
       "created_at                                                                            \n",
       "2025-05-21 09:28:20.378074+00:00  Assistant Thought: We either could not find so...   \n",
       "2025-05-21 10:49:25.181414+00:00                 Hello! How can I assist you today?   \n",
       "2025-05-21 10:49:28.188575+00:00  Assistant Thought: This context has sufficient...   \n",
       "2025-05-21 10:49:29.855939+00:00  Assistant Thought: We either could not find so...   \n",
       "2025-05-21 10:49:31.534413+00:00  Assistant Thought: We either could not find so...   \n",
       "\n",
       "                                  input_tokens  output_tokens        model  \\\n",
       "created_at                                                                   \n",
       "2025-05-21 09:28:20.378074+00:00           869             41  gpt-4o-mini   \n",
       "2025-05-21 10:49:25.181414+00:00             8              9       gpt-4o   \n",
       "2025-05-21 10:49:28.188575+00:00           649            132  gpt-4o-mini   \n",
       "2025-05-21 10:49:29.855939+00:00           809             32  gpt-4o-mini   \n",
       "2025-05-21 10:49:31.534413+00:00           869             41  gpt-4o-mini   \n",
       "\n",
       "                                                         inference_params  \\\n",
       "created_at                                                                  \n",
       "2025-05-21 09:28:20.378074+00:00  {'stop': ['[END]'], 'temperature': 0.0}   \n",
       "2025-05-21 10:49:25.181414+00:00       {'stop': None, 'temperature': 0.5}   \n",
       "2025-05-21 10:49:28.188575+00:00  {'stop': ['[END]'], 'temperature': 0.0}   \n",
       "2025-05-21 10:49:29.855939+00:00  {'stop': ['[END]'], 'temperature': 0.0}   \n",
       "2025-05-21 10:49:31.534413+00:00  {'stop': ['[END]'], 'temperature': 0.0}   \n",
       "\n",
       "                                  is_openai  app  \\\n",
       "created_at                                         \n",
       "2025-05-21 09:28:20.378074+00:00       True  RAG   \n",
       "2025-05-21 10:49:25.181414+00:00       True  RAG   \n",
       "2025-05-21 10:49:28.188575+00:00       True  RAG   \n",
       "2025-05-21 10:49:29.855939+00:00       True  RAG   \n",
       "2025-05-21 10:49:31.534413+00:00       True  RAG   \n",
       "\n",
       "                                                        created_at  \n",
       "created_at                                                          \n",
       "2025-05-21 09:28:20.378074+00:00  2025-05-21T09:28:20.378074+00:00  \n",
       "2025-05-21 10:49:25.181414+00:00  2025-05-21T10:49:25.181414+00:00  \n",
       "2025-05-21 10:49:28.188575+00:00  2025-05-21T10:49:28.188575+00:00  \n",
       "2025-05-21 10:49:29.855939+00:00  2025-05-21T10:49:29.855939+00:00  \n",
       "2025-05-21 10:49:31.534413+00:00  2025-05-21T10:49:31.534413+00:00  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = supabase.table('cost_projecting').select(\"*\").eq('app', 'RAG').execute()\n",
    "completions_df = pd.DataFrame(response.data)\n",
    "completions_df.index = pd.to_datetime(completions_df['created_at'])\n",
    "\n",
    "completions_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a120b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost for the completions dataframe:\n",
      "GPT-3.5-Turbo cost: $0.019285\n",
      "GPT-4o-mini cost: $0.019285\n"
     ]
    }
   ],
   "source": [
    "prices = { # per 1M tokens\n",
    "    'gpt-3.5-turbo': {\n",
    "        'prompt': 0.5,\n",
    "        'completion': 1.5\n",
    "    },\n",
    "    'gpt-4o-mini': {\n",
    "        'prompt': 0.15,\n",
    "        'completion': 0.6\n",
    "    },\n",
    "    'gpt-4o': {\n",
    "        'prompt': 3,\n",
    "        'completion': 6\n",
    "    },\n",
    "    'gpt-4': {\n",
    "        'prompt': 10,\n",
    "        'completion': 30\n",
    "    },\n",
    "    'gpt-4-turbo': {\n",
    "        'prompt': 5,\n",
    "        'completion': 15\n",
    "    },\n",
    "    'text-embedding-3-small': {\n",
    "        'prompt': 0.02,\n",
    "        'completion': 0\n",
    "    },\n",
    "    'text-embedding-3-large': {\n",
    "        'prompt': 0.13,\n",
    "        'completion': 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# generate current cost for the completions dataframe\n",
    "\n",
    "total_input_tokens = completions_df['input_tokens'].sum()\n",
    "total_output_tokens = completions_df['output_tokens'].sum()\n",
    "\n",
    "\n",
    "def calculate_cost(input_tokens, output_tokens, model):\n",
    "    if model not in prices:\n",
    "        return None\n",
    "\n",
    "    prompt_cost = input_tokens / 1e6\n",
    "    completion_cost = output_tokens / 1e6\n",
    "\n",
    "    return prompt_cost + completion_cost\n",
    "\n",
    "print('Total cost for the completions dataframe:')\n",
    "costs = calculate_cost(total_input_tokens, total_output_tokens, 'gpt-3.5-turbo'), calculate_cost(total_input_tokens, total_output_tokens, 'gpt-4o-mini')    \n",
    "print(f\"GPT-3.5-Turbo cost: ${float(costs[0]):.6f}\")\n",
    "print(f\"GPT-4o-mini cost: ${float(costs[1]):.6f}\")\n",
    "# calculate_cost(354, 400, 'gpt-3.5-turbo'), calculate_cost(354, 400, 'gpt-4o')\n",
    "# calculate_cost(total_input_tokens, total_output_tokens, 'gpt-4o-mini'), calculate_cost(total_input_tokens, total_output_tokens, 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d940534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run calculate_cost over every row\n",
    "completions_df['cost'] = completions_df.apply(\n",
    "    lambda row: calculate_cost(row['input_tokens'], row['output_tokens'], row['model']), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7252080a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "created_at",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "cost",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8b7ca348-9450-46c0-a38d-c93c1c709908",
       "rows": [
        [
         "2025-05-19 00:00:00+00:00",
         "0.007495"
        ],
        [
         "2025-05-26 00:00:00+00:00",
         "0.01179"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "created_at\n",
       "2025-05-19 00:00:00+00:00    0.007495\n",
       "2025-05-26 00:00:00+00:00    0.011790\n",
       "Freq: W-MON, Name: cost, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions_df['cost'].resample('W-Mon').sum().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7ff2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='created_at'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completions_df['cost'].resample('W-Mon').sum().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b038640",
   "metadata": {},
   "source": [
    "# Using Llama-3 8b as our Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d79553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.environ.get('HUGGINGFACE_TOKEN'))\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# Mistral's open model (similar performance to Llama-3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# Or smaller but capable models\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2006edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "#     tokenizer.convert_tokens_to_ids(\"assistant\"),\n",
    "# ]\n",
    "\n",
    "# def test_prompt_llama_3_8b(prompt, suppress=False, **kwargs):\n",
    "\n",
    "#     API_URL = \"https://my03m9749ssz7t6h.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "#     headers = {\n",
    "#     \t\"Accept\" : \"application/json\",\n",
    "#     \t\"Authorization\": f\"Bearer {userdata.get('HUGGINGFACE_TOKEN')}\",\n",
    "#     \t\"Content-Type\": \"application/json\"\n",
    "#     }\n",
    "\n",
    "#     llama_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "#     def query(payload):\n",
    "#     \tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "#     \treturn response.json()\n",
    "\n",
    "#     kwargs[\"return_text\"] = False\n",
    "#     kwargs[\"return_full_text\"] = False\n",
    "#     kwargs['max_new_tokens'] = 512\n",
    "#     kwargs['stop'] = [\"<|end_of_text|>\", \"<|eot_id|>\"]\n",
    "\n",
    "#     output = query({\n",
    "#     \t\"inputs\": llama_prompt,\n",
    "#     \t\"parameters\": kwargs\n",
    "#     })\n",
    "#     answer = output[0]['generated_text']\n",
    "#     if not suppress:\n",
    "#         print(f'PROMPT:\\n------\\n{llama_prompt}\\n------\\nRESPONSE\\n------\\n{answer}')\n",
    "#     else:\n",
    "#         return answer\n",
    "\n",
    "# test_prompt_llama_3_8b('1+1=?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b4d0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prompt_llama_3_8b('1+1=?', suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81a0f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaChatLLM(BaseModel):\n",
    "    temperature: float = 0.3\n",
    "    do_sample: bool = True\n",
    "    max_new_tokens: int = 256\n",
    "\n",
    "    def generate(self, prompt: str, stop: List[str] = None):\n",
    "        response = test_prompt_llama_3_8b(prompt, suppress=True)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee3474",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llama_rag = RagBot(llm=\u001b[43mllama_llm\u001b[49m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m, stop_pattern=[\u001b[33m'\u001b[39m\u001b[33m[END]\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(llama_rag.run(\u001b[33m'\u001b[39m\u001b[33mI want to go to a museum in Munich\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'llama_llm' is not defined"
     ]
    }
   ],
   "source": [
    "# llama_rag = RagBot(llm=llama_llm, verbose=False, stop_pattern=['[END]'])\n",
    "# print(llama_rag.run('I want to go to a museum in Munich'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f8f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_rag.user_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d813ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_rag.ai_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_rag.contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a427687",
   "metadata": {},
   "source": [
    "## Using Ollama locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6c7709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "class OllamaLLM(BaseModel):\n",
    "    model_name:str = \"llama3.2\"\n",
    "\n",
    "    def generate(self, prompt: str, stop: List[str] = None):\n",
    "        messages = [{'role': 'user', 'content': prompt}]\n",
    "        response = ollama.chat(model=self.model_name, messages=messages, options={'stop': stop})\n",
    "        return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abe0eed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 = 2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run ollama locally\n",
    "# ollama run llama3.2\n",
    "\n",
    "o_llama = OllamaLLM()\n",
    "o_llama.generate('What is 1+1?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b6466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Es gibt viele Museen in Deutschland. Welches Museum sind Sie gerne besuchen? (There are many museums in Germany. Which museum would you like to visit?)\n",
      "[END]\n"
     ]
    }
   ],
   "source": [
    "o_llama_rag = RagBot(llm=o_llama, verbose=False, stop_pattern=[])  \n",
    "print(o_llama_rag.run('Ich will Museum besuchen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14cc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[END]\\n[START]\\nUser Input: Ich will Museum besuchen\\nContext: NO CONTEXT FOUND\\nContext URL: NONE\\nContext Score : 0\\nAssistant Thought: We either could not find something or we don't need to look something up\\nAssistant Response: Ich empfehle dir, einen Museum in deiner Nähe zu suchen. Du kannst dies online tun oder dich nach Empfehlungen stellen.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_llama_rag.ai_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START]\n",
      "User Input: Ich will Museum besuchen\n",
      "Context: NO CONTEXT FOUND\n",
      "Context URL: NONE\n",
      "Context Score: 0\n",
      "[END]\n",
      "[START]\n",
      "User Input: Ich will Museum besuchen\n",
      "Context: NO CONTEXT FOUND\n",
      "Context URL: NONE\n",
      "Context Score : 0\n",
      "Assistant Thought: We either could not find something or we don't need to look something up\n",
      "Assistant Response: Ich empfehle dir, einen Museum in deiner Nähe zu suchen. Du kannst dies online tun oder dich nach Empfehlungen stellen.\n",
      "[END]\n"
     ]
    }
   ],
   "source": [
    "print(o_llama_rag.running_convo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d56947",
   "metadata": {},
   "source": [
    "## Using Command-R as our Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42ace6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (0.45.5)\n",
      "Requirement already satisfied: accelerate in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: torch[transformers] in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from bitsandbytes) (2.2.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from torch[transformers]) (80.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from accelerate) (0.31.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: requests in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch[transformers]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from jinja2->torch[transformers]) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kadda\\github_repos\\rag-gen-ai\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: torch 2.7.0+cu128 does not provide the extra 'transformers'\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes accelerate torch[transformers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f09da46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># Safety Preamble\n",
      "The instructions in this section override those in the task description and style guide sections. Don't answer questions that are harmful or immoral.\n",
      "\n",
      "# System Preamble\n",
      "## Basic Rules\n",
      "You are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user's requests, you cite your sources in your answers, according to those instructions.\n",
      "\n",
      "# User Preamble\n",
      "## Task and Context\n",
      "You help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user's needs as best you can, which will be wide-ranging.\n",
      "\n",
      "## Style Guide\n",
      "Unless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Whats the biggest penguin in the world?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\n",
      "Document: 0\n",
      "title: Tall penguins\n",
      "text: Emperor penguins are the tallest growing up to 122 cm in height.\n",
      "\n",
      "Document: 1\n",
      "title: Penguin habitats\n",
      "text: Emperor penguins only live in Antarctica.\n",
      "</results><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Carefully perform the following instructions, in order, starting each with a new line.\n",
      "Firstly, Decide which of the retrieved documents are relevant to the user's last input by writing 'Relevant Documents:' followed by comma-separated list of document numbers. If none are relevant, you should instead write 'None'.\n",
      "Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user's last input by writing 'Cited Documents:' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write 'None'.\n",
      "Thirdly, Write 'Answer:' followed by a response to the user's last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\n",
      "Finally, Write 'Grounded answer:' followed by a response to the user's last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"CohereForAI/c4ai-command-r-v01\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# define conversation input:\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Whats the biggest penguin in the world?\"}\n",
    "]\n",
    "# define documents to ground on:\n",
    "documents = [\n",
    "    { \"title\": \"Tall penguins\", \"text\": \"Emperor penguins are the tallest growing up to 122 cm in height.\" },\n",
    "    { \"title\": \"Penguin habitats\", \"text\": \"Emperor penguins only live in Antarctica.\"}\n",
    "]\n",
    "\n",
    "# render the tool use prompt as a string:\n",
    "grounded_generation_prompt = tokenizer.apply_grounded_generation_template(\n",
    "    conversation,\n",
    "    documents=documents,\n",
    "    citation_mode=\"accurate\", # or \"fast\"\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "print(grounded_generation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3655ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 579])\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "grounded_generation_tokens = tokenizer.apply_grounded_generation_template(\n",
    "    conversation,\n",
    "    documents=documents,\n",
    "    citation_mode=\"accurate\", # or \"fast\"\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(grounded_generation_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9032017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Verify CUDA setup\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b273266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 2070\n",
      "Operation successful on device: cuda:0\n",
      "CUDA is working properly!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Basic CUDA info\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Test with a simple tensor operation\n",
    "if torch.cuda.is_available():\n",
    "    # Create tensors on GPU\n",
    "    x = torch.rand(5, 3).cuda()\n",
    "    y = torch.rand(5, 3).cuda()\n",
    "    \n",
    "    # Perform computation\n",
    "    z = x + y\n",
    "    \n",
    "    print(f\"Operation successful on device: {z.device}\")\n",
    "    print(\"CUDA is working properly!\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e5b114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CUDA memory: 8.59 GB\n",
      "Allocated CUDA memory: 0.00 GB\n",
      "Reserved CUDA memory: 15.79 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Show available memory\n",
    "print(f\"Total CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "print(f\"Allocated CUDA memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved CUDA memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfabbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 13.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mCohereLabs/c4ai-command-r-v01-4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:765\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    763\u001b[39m     param = file_pointer.get_slice(serialized_param_name)\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     param = \u001b[43mempty_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n\u001b[32m    767\u001b[39m to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[32m    768\u001b[39m     model,\n\u001b[32m    769\u001b[39m     param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    772\u001b[39m     hf_quantizer,\n\u001b[32m    773\u001b[39m )\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# In this case, the param is already on the correct device!\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 13.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"CohereLabs/c4ai-command-r-v01-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc09ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 12.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mCohereForAI/c4ai-command-r-v01-4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:765\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    763\u001b[39m     param = file_pointer.get_slice(serialized_param_name)\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     param = \u001b[43mempty_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_device\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# It is actually not empty!\u001b[39;00m\n\u001b[32m    767\u001b[39m to_contiguous, casting_dtype = _infer_parameter_dtype(\n\u001b[32m    768\u001b[39m     model,\n\u001b[32m    769\u001b[39m     param_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    772\u001b[39m     hf_quantizer,\n\u001b[32m    773\u001b[39m )\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# In this case, the param is already on the correct device!\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.70 GiB is allocated by PyTorch, and 12.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# pip install 'transformers>=4.39.1' bitsandbytes accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd429d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 4605349888.0 bytes required\n",
      "  - cpu: 4605349888.0 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set logging to see detailed errors\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Use specific parameters for 4-bit quantized loading\n",
    "model_id = \"CohereLabs/c4ai-command-r-v01-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model with explicit quantization parameters\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",  # Automatically place tensors on appropriate devices\n",
    "        # load_in_4bit=True,  # Explicitly use 4-bit quantization\n",
    "        # torch_dtype=torch.bfloat16,  # Use bfloat16 precision\n",
    "        # trust_remote_code=True,  # Needed for some models with custom code\n",
    "        # use_cache=True  # Enable KV caching for faster inference\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "645bdd0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Hand-craft which layers go on GPU (in 4-bit) vs CPU (FP32).\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Adjust the cutoff (here 8) based on how much GPU RAM you actually have.\u001b[39;00m\n\u001b[32m     22\u001b[39m device_map = {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformer.wte\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtransformer.wpe\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlm_head\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m7GiB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m60GiB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Loaded with 4-bit on GPU and FP32 offload on CPU\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4228\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4225\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4228\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4235\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4236\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kadda\\github_repos\\rag-gen-ai\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:104\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    105\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m         )\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.39.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch, logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_id = \"CohereLabs/c4ai-command-r-v01-4bit\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_enable_fp32_cpu_offload=True,   # keep offloaded 4-bit modules in FP32 on CPU\n",
    "    llm_int8_enable_fp32_cpu_offload=True     # required by the validator\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Hand-craft which layers go on GPU (in 4-bit) vs CPU (FP32).\n",
    "# Adjust the cutoff (here 8) based on how much GPU RAM you actually have.\n",
    "device_map = {\n",
    "    \"transformer.wte\": 0,\n",
    "    \"transformer.wpe\": 0,\n",
    "    **{f\"transformer.h.{i}\": 0 for i in range(8)},    # first 8 blocks → GPU\n",
    "    **{f\"transformer.h.{i}\": \"cpu\" for i in range(8, 24)},  # rest → CPU\n",
    "    \"transformer.ln_f\": \"cpu\",\n",
    "    \"lm_head\": \"cpu\",\n",
    "}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    max_memory={0: \"7GiB\", \"cpu\": \"60GiB\"},\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"✅ Loaded with 4-bit on GPU and FP32 offload on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2289c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(\n",
    "    grounded_generation_tokens,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_command_r(documents):\n",
    "    return [{'title': f'Document {index + 1}', 'text': document['metadata']['text']} for index, document in enumerate(documents)]\n",
    "\n",
    "command_r_docs = format_for_command_r(query_from_pinecone('I want to visit a Musuem in Munich', top_k=3, include_metadata=True))\n",
    "\n",
    "len(command_r_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
